{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bebe0e2",
   "metadata": {},
   "source": [
    "# ü§ñ AI Research Assistant with LangGraph\n",
    "\n",
    "A modular, extensible research assistant using LangGraph, BM25 retrieval, and Groq LLM.\n",
    "\n",
    "## ‚ú® Key Features\n",
    "\n",
    "- **Object-Oriented Design**: Clean agent classes with clear responsibilities\n",
    "- **Design Patterns**: Strategy, Builder, Facade, and Agent patterns for extensibility\n",
    "- **Flexible Pipelines**: Easy to customize agent workflows\n",
    "- **BM25 Retrieval**: Fast lexical search over PDF documents\n",
    "- **Three-Agent System**: Researcher ‚Üí Reviewer ‚Üí Synthesizer\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "1. Run all cells in order\n",
    "2. Use the examples to see the system in action\n",
    "3. Customize pipelines using the builder pattern\n",
    "4. Test your own queries with `quick_research()`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ac77d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "from typing import List, Dict, Any, TypedDict, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Document loading & splitting\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# LLM\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# BM25\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0e4016",
   "metadata": {},
   "source": [
    "# LangGraph Research Assistant with Design Patterns\n",
    "\n",
    "This notebook demonstrates:\n",
    "- **Strategy Pattern**: Different retrieval strategies (BM25, could add semantic, hybrid, etc.)\n",
    "- **Agent Pattern**: Each agent is a class with a defined interface\n",
    "- **Builder Pattern**: For constructing the research pipeline\n",
    "- Modular, extensible architecture for easy customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61866e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Configuration\n",
    "# ---------------------------\n",
    "FILES_DIR = \"../hackathon - Copie/files\"  # folder containing PDFs\n",
    "os.environ[\"GROQ_API_KEY\"] = \"put-apikey-here\"\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e89d9",
   "metadata": {},
   "source": [
    "## Step 1: Define Base Classes and Interfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9513af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# ---------------------------\n",
    "# Simple tokenizer for BM25\n",
    "# ---------------------------\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Tokenize text for BM25 indexing\"\"\"\n",
    "    tokens = re.findall(r\"\\w+\", text.lower())\n",
    "    return [t for t in tokens if len(t) > 1]\n",
    "\n",
    "# ---------------------------\n",
    "# Data classes\n",
    "# ---------------------------\n",
    "@dataclass\n",
    "class DocChunk:\n",
    "    \"\"\"Represents a document chunk with content and metadata\"\"\"\n",
    "    page_content: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class ResearchState:\n",
    "    \"\"\"State object passed between agents\"\"\"\n",
    "    topic: str\n",
    "    summary: Optional[str] = None\n",
    "    critique: Optional[str] = None\n",
    "    insight: Optional[str] = None\n",
    "    sources: Optional[List[str]] = None\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert to dictionary for LangGraph\"\"\"\n",
    "        return {\n",
    "            \"topic\": self.topic,\n",
    "            \"summary\": self.summary,\n",
    "            \"critique\": self.critique,\n",
    "            \"insight\": self.insight,\n",
    "            \"sources\": self.sources\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: dict) -> 'ResearchState':\n",
    "        \"\"\"Create from dictionary\"\"\"\n",
    "        return cls(**data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebb167f",
   "metadata": {},
   "source": [
    "## Step 2: Strategy Pattern - Retrieval Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bc8500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Strategy Pattern: Retrieval Interface\n",
    "# ---------------------------\n",
    "class RetrievalStrategy(ABC):\n",
    "    \"\"\"Abstract base class for retrieval strategies\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[DocChunk]:\n",
    "        \"\"\"Retrieve relevant documents for a query\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_strategy_name(self) -> str:\n",
    "        \"\"\"Return the name of this strategy\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class BM25RetrievalStrategy(RetrievalStrategy):\n",
    "    \"\"\"BM25 lexical retrieval strategy\"\"\"\n",
    "    \n",
    "    def __init__(self, chunks: List[DocChunk]):\n",
    "        self.chunks = chunks\n",
    "        self.tokenized_texts = [simple_tokenize(c.page_content) for c in chunks]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_texts)\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 3) -> List[DocChunk]:\n",
    "        \"\"\"Retrieve documents using BM25 ranking\"\"\"\n",
    "        q_tokens = simple_tokenize(query)\n",
    "        if not q_tokens:\n",
    "            return []\n",
    "        \n",
    "        scores = self.bm25.get_scores(q_tokens)\n",
    "        idx_scores = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "        top = [i for i, sc in idx_scores[:k] if sc > 0]\n",
    "        \n",
    "        if not top and len(idx_scores) > 0:\n",
    "            top = [i for i, _ in idx_scores[:k]]\n",
    "        \n",
    "        return [self.chunks[i] for i in top]\n",
    "    \n",
    "    def get_strategy_name(self) -> str:\n",
    "        return \"BM25 Lexical Retrieval\"\n",
    "\n",
    "\n",
    "# Placeholder for future strategies\n",
    "class SemanticRetrievalStrategy(RetrievalStrategy):\n",
    "    \"\"\"Placeholder for semantic/embedding-based retrieval\"\"\"\n",
    "    \n",
    "    def __init__(self, chunks: List[DocChunk]):\n",
    "        self.chunks = chunks\n",
    "        # TODO: Initialize embeddings\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 3) -> List[DocChunk]:\n",
    "        # TODO: Implement semantic search\n",
    "        raise NotImplementedError(\"Semantic retrieval not yet implemented\")\n",
    "    \n",
    "    def get_strategy_name(self) -> str:\n",
    "        return \"Semantic Embedding Retrieval\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f4849",
   "metadata": {},
   "source": [
    "## Step 3: Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ecb9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Document Loader Class\n",
    "# ---------------------------\n",
    "class DocumentLoader:\n",
    "    \"\"\"Handles loading and chunking of documents\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "    \n",
    "    def load_pdfs(self, files_dir: str) -> List[DocChunk]:\n",
    "        \"\"\"Load and chunk all PDFs from a directory\"\"\"\n",
    "        chunks: List[DocChunk] = []\n",
    "        pdf_paths = sorted(glob(os.path.join(files_dir, \"*.pdf\")))\n",
    "        \n",
    "        print(f\"üì• Loading {len(pdf_paths)} PDF(s)...\")\n",
    "        \n",
    "        for pdf_path in pdf_paths:\n",
    "            filename = os.path.basename(pdf_path)\n",
    "            try:\n",
    "                loader = PyPDFLoader(pdf_path)\n",
    "                docs = loader.load()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Warning: failed to load {pdf_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Add metadata\n",
    "            for i, d in enumerate(docs):\n",
    "                if not d.metadata:\n",
    "                    d.metadata = {}\n",
    "                d.metadata[\"source\"] = filename\n",
    "                d.metadata[\"orig_page_index\"] = d.metadata.get(\"page\", i)\n",
    "            \n",
    "            # Split into chunks\n",
    "            doc_chunks = self.splitter.split_documents(docs)\n",
    "            for idx, c in enumerate(doc_chunks):\n",
    "                meta = dict(c.metadata)\n",
    "                meta[\"chunk_id\"] = f\"{filename}__chunk{idx}\"\n",
    "                chunks.append(DocChunk(page_content=c.page_content, metadata=meta))\n",
    "        \n",
    "        print(f\"‚úÖ Loaded and chunked {len(chunks)} chunks from {len(pdf_paths)} PDF(s).\")\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c21c638",
   "metadata": {},
   "source": [
    "## Step 4: Agent Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28310bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Agent Base Class\n",
    "# ---------------------------\n",
    "class Agent(ABC):\n",
    "    \"\"\"Abstract base class for all agents\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, name: str):\n",
    "        self.llm = llm\n",
    "        self.name = name\n",
    "    \n",
    "    @abstractmethod\n",
    "    def process(self, state: dict) -> dict:\n",
    "        \"\"\"Process the state and return updates\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, state: dict) -> dict:\n",
    "        \"\"\"Make agent callable for LangGraph\"\"\"\n",
    "        return self.process(state)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Researcher Agent\n",
    "# ---------------------------\n",
    "class ResearcherAgent(Agent):\n",
    "    \"\"\"Agent that retrieves and summarizes relevant documents\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, retrieval_strategy: RetrievalStrategy, k: int = 4):\n",
    "        super().__init__(llm, \"Researcher\")\n",
    "        self.retrieval_strategy = retrieval_strategy\n",
    "        self.k = k\n",
    "    \n",
    "    def process(self, state: dict) -> dict:\n",
    "        \"\"\"Retrieve documents and create summary\"\"\"\n",
    "        topic = state.get(\"topic\", \"\").strip()\n",
    "        \n",
    "        if not topic:\n",
    "            return {\"summary\": \"No topic provided.\"}\n",
    "        \n",
    "        # Retrieve documents\n",
    "        docs = self.retrieval_strategy.retrieve(topic, k=self.k)\n",
    "        \n",
    "        if not docs:\n",
    "            return {\"summary\": \"No relevant documents found.\"}\n",
    "        \n",
    "        # Prepare context\n",
    "        context_pieces = []\n",
    "        sources = []\n",
    "        \n",
    "        for d in docs:\n",
    "            snippet = d.page_content.strip()\n",
    "            if len(snippet) > 800:\n",
    "                snippet = snippet[:800].rsplit(\" \", 1)[0] + \" ...\"\n",
    "            \n",
    "            source = d.metadata.get(\"source\", \"unknown\")\n",
    "            chunk_id = d.metadata.get(\"chunk_id\", \"\")\n",
    "            context_pieces.append(f\"[SOURCE: {source} | CHUNK: {chunk_id}]\\n{snippet}\")\n",
    "            sources.append(source)\n",
    "        \n",
    "        context = \"\\n\\n---\\n\\n\".join(context_pieces)\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = (\n",
    "            f\"You are a research assistant. The user asked about: '{topic}'.\\n\\n\"\n",
    "            f\"Read the following retrieved excerpts (using {self.retrieval_strategy.get_strategy_name()}) \"\n",
    "            f\"and produce a concise summary of the main findings or facts relevant to the topic. \"\n",
    "            f\"Be explicit about which sources support which points.\\n\\n\"\n",
    "            f\"EXCERPTS:\\n\\n{context}\\n\\n\"\n",
    "            \"Return a short summary and a short list of (source -> supporting sentence).\"\n",
    "        )\n",
    "        \n",
    "        # Get LLM response\n",
    "        resp = self.llm.invoke(prompt)\n",
    "        summary_text = getattr(resp, \"content\", None) or str(resp)\n",
    "        \n",
    "        return {\n",
    "            \"summary\": summary_text, \n",
    "            \"sources\": list(dict.fromkeys(sources))\n",
    "        }\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Reviewer Agent\n",
    "# ---------------------------\n",
    "class ReviewerAgent(Agent):\n",
    "    \"\"\"Agent that critically reviews the research summary\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        super().__init__(llm, \"Reviewer\")\n",
    "    \n",
    "    def process(self, state: dict) -> dict:\n",
    "        \"\"\"Review and critique the summary\"\"\"\n",
    "        summary = state.get(\"summary\", \"\")\n",
    "        \n",
    "        if not summary:\n",
    "            return {\"critique\": \"No summary to review.\"}\n",
    "        \n",
    "        prompt = (\n",
    "            \"You are a critical reviewer. Read the following summary and point out: \"\n",
    "            \"1) statements that lack direct support from the provided excerpts, \"\n",
    "            \"2) possible biases or missing considerations, and \"\n",
    "            \"3) questions or follow-ups to verify the claims.\\n\\n\"\n",
    "            f\"SUMMARY:\\n\\n{summary}\\n\\n\"\n",
    "            \"Give your critique in bullet points.\"\n",
    "        )\n",
    "        \n",
    "        resp = self.llm.invoke(prompt)\n",
    "        critique_text = getattr(resp, \"content\", None) or str(resp)\n",
    "        \n",
    "        return {\"critique\": critique_text}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Synthesizer Agent\n",
    "# ---------------------------\n",
    "class SynthesizerAgent(Agent):\n",
    "    \"\"\"Agent that synthesizes insights from research and review\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        super().__init__(llm, \"Synthesizer\")\n",
    "    \n",
    "    def process(self, state: dict) -> dict:\n",
    "        \"\"\"Synthesize final insights\"\"\"\n",
    "        summary = state.get(\"summary\", \"\")\n",
    "        critique = state.get(\"critique\", \"\")\n",
    "        sources = state.get(\"sources\", [])\n",
    "        \n",
    "        prompt = (\n",
    "            \"You are a synthesizer. Combine the summary and critique into a 'Collective Insight Report'. \"\n",
    "            \"Include: a 2-3 sentence insight, 2 testable hypotheses or follow-up experiments, and which sources \"\n",
    "            \"would be most relevant to test those hypotheses. Keep it concise.\\n\\n\"\n",
    "            f\"SUMMARY:\\n{summary}\\n\\nCRITIQUE:\\n{critique}\\n\\nSOURCES:\\n{', '.join(sources)}\"\n",
    "        )\n",
    "        \n",
    "        resp = self.llm.invoke(prompt)\n",
    "        insight_text = getattr(resp, \"content\", None) or str(resp)\n",
    "        \n",
    "        return {\"insight\": insight_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e083de1",
   "metadata": {},
   "source": [
    "## Step 5: Builder Pattern - Research Pipeline Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dda11121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Builder Pattern: Pipeline Builder\n",
    "# ---------------------------\n",
    "class ResearchPipelineBuilder:\n",
    "    \"\"\"Builder for constructing research pipelines with different configurations\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.retrieval_strategy = None\n",
    "        self.agents = []\n",
    "        self.graph_config = []\n",
    "    \n",
    "    def with_retrieval_strategy(self, strategy: RetrievalStrategy):\n",
    "        \"\"\"Set the retrieval strategy\"\"\"\n",
    "        self.retrieval_strategy = strategy\n",
    "        return self\n",
    "    \n",
    "    def with_researcher(self, k: int = 4):\n",
    "        \"\"\"Add researcher agent\"\"\"\n",
    "        if not self.retrieval_strategy:\n",
    "            raise ValueError(\"Retrieval strategy must be set before adding researcher\")\n",
    "        \n",
    "        researcher = ResearcherAgent(self.llm, self.retrieval_strategy, k=k)\n",
    "        self.agents.append((\"researcher\", researcher))\n",
    "        return self\n",
    "    \n",
    "    def with_reviewer(self):\n",
    "        \"\"\"Add reviewer agent\"\"\"\n",
    "        reviewer = ReviewerAgent(self.llm)\n",
    "        self.agents.append((\"reviewer\", reviewer))\n",
    "        return self\n",
    "    \n",
    "    def with_synthesizer(self):\n",
    "        \"\"\"Add synthesizer agent\"\"\"\n",
    "        synthesizer = SynthesizerAgent(self.llm)\n",
    "        self.agents.append((\"synthesizer\", synthesizer))\n",
    "        return self\n",
    "    \n",
    "    def build(self):\n",
    "        \"\"\"Build the LangGraph pipeline\"\"\"\n",
    "        if not self.agents:\n",
    "            raise ValueError(\"No agents added to pipeline\")\n",
    "        \n",
    "        # Create state schema\n",
    "        class PipelineState(TypedDict):\n",
    "            topic: str\n",
    "            summary: Optional[str]\n",
    "            critique: Optional[str]\n",
    "            insight: Optional[str]\n",
    "            sources: Optional[List[str]]\n",
    "        \n",
    "        # Create graph\n",
    "        graph = StateGraph(PipelineState)\n",
    "        \n",
    "        # Add nodes\n",
    "        for name, agent in self.agents:\n",
    "            graph.add_node(name, agent)\n",
    "        \n",
    "        # Add edges (sequential for now)\n",
    "        for i in range(len(self.agents) - 1):\n",
    "            graph.add_edge(self.agents[i][0], self.agents[i + 1][0])\n",
    "        \n",
    "        # Set entry point\n",
    "        graph.set_entry_point(self.agents[0][0])\n",
    "        \n",
    "        return graph.compile()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Research Assistant (Facade Pattern)\n",
    "# ---------------------------\n",
    "class ResearchAssistant:\n",
    "    \"\"\"High-level interface for the research system\"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline, retrieval_strategy: RetrievalStrategy):\n",
    "        self.pipeline = pipeline\n",
    "        self.retrieval_strategy = retrieval_strategy\n",
    "    \n",
    "    def research(self, topic: str) -> dict:\n",
    "        \"\"\"Perform research on a topic\"\"\"\n",
    "        print(f\"üî¨ Researching: {topic}\")\n",
    "        print(f\"üìä Using: {self.retrieval_strategy.get_strategy_name()}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        result = self.pipeline.invoke({\"topic\": topic})\n",
    "        return result\n",
    "    \n",
    "    def print_results(self, result: dict):\n",
    "        \"\"\"Pretty print research results\"\"\"\n",
    "        topic = result.get(\"topic\", \"Unknown\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"üìù RESEARCH REPORT: {topic}\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "        \n",
    "        print(\"üìò RESEARCHER SUMMARY:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(result.get(\"summary\", \"‚Äî\"))\n",
    "        print()\n",
    "        \n",
    "        print(\"\\nüîç REVIEWER CRITIQUE:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(result.get(\"critique\", \"‚Äî\"))\n",
    "        print()\n",
    "        \n",
    "        print(\"\\nüí° COLLECTIVE INSIGHT:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(result.get(\"insight\", \"‚Äî\"))\n",
    "        print()\n",
    "        \n",
    "        sources = result.get(\"sources\", [])\n",
    "        if sources:\n",
    "            print(\"\\nüìö SOURCES USED:\")\n",
    "            print(\"-\" * 80)\n",
    "            for i, source in enumerate(sources, 1):\n",
    "                print(f\"{i}. {source}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74010dd7",
   "metadata": {},
   "source": [
    "## Step 6: Initialize the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fee2833d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading 5 PDF(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not convert string to float: b'0.00-9999999' : FloatObject (b'0.00-9999999') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.00-9999999' : FloatObject (b'0.00-9999999') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded and chunked 262 chunks from 5 PDF(s).\n",
      "‚úÖ System ready with 262 document chunks\n"
     ]
    }
   ],
   "source": [
    "# Load documents\n",
    "loader = DocumentLoader(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = loader.load_pdfs(FILES_DIR)\n",
    "\n",
    "# Create retrieval strategy\n",
    "bm25_strategy = BM25RetrievalStrategy(chunks) if chunks else None\n",
    "\n",
    "if not bm25_strategy:\n",
    "    print(\"‚ö†Ô∏è  No documents loaded. Please add PDFs to the files directory.\")\n",
    "else:\n",
    "    print(f\"‚úÖ System ready with {len(chunks)} document chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b50e36df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Research Assistant is ready!\n",
      "üìä Pipeline: Researcher ‚Üí Reviewer ‚Üí Synthesizer\n",
      "üîç Strategy: BM25 Lexical Retrieval\n"
     ]
    }
   ],
   "source": [
    "# Build the research pipeline using the builder pattern\n",
    "pipeline = (ResearchPipelineBuilder(llm)\n",
    "    .with_retrieval_strategy(bm25_strategy)\n",
    "    .with_researcher(k=4)\n",
    "    .with_reviewer()\n",
    "    .with_synthesizer()\n",
    "    .build())\n",
    "\n",
    "# Create the research assistant\n",
    "assistant = ResearchAssistant(pipeline, bm25_strategy)\n",
    "\n",
    "print(\"ü§ñ Research Assistant is ready!\")\n",
    "print(\"üìä Pipeline: Researcher ‚Üí Reviewer ‚Üí Synthesizer\")\n",
    "print(f\"üîç Strategy: {bm25_strategy.get_strategy_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7975205a",
   "metadata": {},
   "source": [
    "## üß™ Example 1: Research on Climate Change and AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c658bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Researching: How is AI being used to combat climate change?\n",
      "üìä Using: BM25 Lexical Retrieval\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "üìù RESEARCH REPORT: How is AI being used to combat climate change?\n",
      "================================================================================\n",
      "\n",
      "üìò RESEARCHER SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "**Summary:** AI is being used to combat climate change through various applications, including integrative emissions monitoring and management for nature-based climate solutions, such as forests. AI techniques, like machine learning and computer vision, can detect wildfires, estimate carbon stock, and support disaster response efforts. However, the development of AI models also has a significant environmental impact, with high energy consumption and emissions. \n",
      "\n",
      "**Source-Sentence List:**\n",
      "- 3_Climate And Resource Awareness is Imperative to Achieving Sustainable AI   and Preventing a Global A.pdf -> \"required 30.84M GPU hours ... with a total of 8,930 tons CO2e in emissions.\"\n",
      "- 2_Towards AI-driven Integrative Emissions Monitoring  Management for   Nature-Based Climate Solutions.pdf -> \"AI techniques have been used to address various issues affecting nature-based climate solutions such as vegetation-based offset projects.\"\n",
      "- 2_Towards AI-driven Integrative Emissions Monitoring  Management for   Nature-Based Climate Solutions.pdf -> \"satellite imagery-based computer vision can be used to detect wildfires in real-time, supporting critical disaster response efforts.\"\n",
      "\n",
      "\n",
      "üîç REVIEWER CRITIQUE:\n",
      "--------------------------------------------------------------------------------\n",
      "Here's a critique of the summary in bullet points:\n",
      "\n",
      "* **Lack of direct support:**\n",
      "  * The statement that AI is being used to \"estimate carbon stock\" is not directly supported by the provided excerpts. While the excerpts mention AI techniques for detecting wildfires and addressing issues in nature-based climate solutions, there is no explicit mention of estimating carbon stock.\n",
      "  * The claim that AI is used for \"integrative emissions monitoring and management for nature-based climate solutions, such as forests\" is partially supported by the excerpt from \"2_Towards AI-driven Integrative Emissions Monitoring  Management for   Nature-Based Climate Solutions.pdf\", but it would be more accurate to say that AI is being used to address issues in nature-based climate solutions, rather than specifically for integrative emissions monitoring and management.\n",
      "\n",
      "* **Possible biases or missing considerations:**\n",
      "  * The summary presents a somewhat balanced view of AI's role in combating climate change, but it may be biased towards highlighting the benefits of AI without fully exploring the complexities of its environmental impact. For example, the summary mentions the high energy consumption and emissions of AI model development, but it does not discuss potential strategies for mitigating these effects.\n",
      "  * The summary focuses on the use of AI in nature-based climate solutions, but it does not consider other potential applications of AI in climate change mitigation, such as optimizing energy consumption in buildings or transportation systems.\n",
      "  * The summary does not address potential limitations or challenges of using AI in climate change mitigation, such as data quality issues, algorithmic biases, or the need for significant computational resources.\n",
      "\n",
      "* **Questions or follow-ups to verify the claims:**\n",
      "  * Can you provide more information on how AI is being used to estimate carbon stock, and what specific techniques or models are being employed?\n",
      "  * How do the benefits of using AI in nature-based climate solutions (e.g. detecting wildfires, supporting disaster response efforts) outweigh the environmental costs of AI model development?\n",
      "  * What strategies are being explored to reduce the environmental impact of AI model development, such as using more energy-efficient hardware or optimizing model architecture?\n",
      "  * How do the results from the \"2_Towards AI-driven Integrative Emissions Monitoring  Management for   Nature-Based Climate Solutions.pdf\" study generalize to other types of nature-based climate solutions, or to other regions or ecosystems?\n",
      "\n",
      "\n",
      "üí° COLLECTIVE INSIGHT:\n",
      "--------------------------------------------------------------------------------\n",
      "**Collective Insight Report**\n",
      "\n",
      "Insight: AI has the potential to combat climate change through applications such as wildfire detection and disaster response, but its development also has a significant environmental impact. However, the current understanding of AI's role in climate change mitigation is limited by a lack of direct support for certain claims and potential biases towards highlighting benefits over complexities. Further research is needed to fully explore the benefits and drawbacks of AI in climate change mitigation.\n",
      "\n",
      "Testable Hypotheses:\n",
      "1. The use of AI in nature-based climate solutions can lead to a net reduction in greenhouse gas emissions, despite the high energy consumption and emissions associated with AI model development.\n",
      "2. The integration of AI techniques, such as machine learning and computer vision, can improve the accuracy and efficiency of carbon stock estimation and wildfire detection in various ecosystems.\n",
      "\n",
      "Relevant Sources:\n",
      "- \"2_Towards AI-driven Integrative Emissions Monitoring  Management for   Nature-Based Climate Solutions.pdf\"\n",
      "- \"3_Climate And Resource Awareness is Imperative to Achieving Sustainable AI   and Preventing a Global A.pdf\"\n",
      "- Additional sources on AI applications in climate change mitigation, such as optimizing energy consumption in buildings or transportation systems, would be necessary to test these hypotheses and provide a more comprehensive understanding of AI's role in combating climate change.\n",
      "\n",
      "\n",
      "üìö SOURCES USED:\n",
      "--------------------------------------------------------------------------------\n",
      "1. 3_Climate And Resource Awareness is Imperative to Achieving Sustainable AI   and Preventing a Global A.pdf\n",
      "2. 2_Towards AI-driven Integrative Emissions Monitoring  Management for   Nature-Based Climate Solutions.pdf\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run research on a specific topic\n",
    "result1 = assistant.research(\"How is AI being used to combat climate change?\")\n",
    "assistant.print_results(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22db45c7",
   "metadata": {},
   "source": [
    "## üß™ Example 2: Research on Machine Learning for Weather Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230222a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = assistant.research(\"What are the applications of machine learning in weather forecasting?\")\n",
    "assistant.print_results(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da18c5",
   "metadata": {},
   "source": [
    "## üß™ Example 3: Building a Custom Pipeline\n",
    "\n",
    "You can easily customize the pipeline by changing the order or configuration of agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fd387c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Researching: Climate data analysis techniques\n",
      "üìä Using: BM25 Lexical Retrieval\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "üìù RESEARCH REPORT: Climate data analysis techniques\n",
      "================================================================================\n",
      "\n",
      "üìò RESEARCHER SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "**Summary:** Climate data analysis techniques involve the use of AI and machine learning methods to analyze and understand the impact of climate on various factors such as disease prevalence, emissions, and resource management. Techniques such as computer vision and deep learning can be used to analyze climate data and support automation and decision-making. Climate data analysis can help identify patterns and relationships between climate factors and disease trends, and can inform strategies for mitigating the impacts of climate change.\n",
      "\n",
      "**Source -> Supporting Sentence:**\n",
      "* 2_Towards AI-driven Integrative Emissions Monitoring  Management for   Nature-Based Climate Solutions.pdf -> \"AI techniques such as computer vision methods can help provide data and support automation aspects e.g. aiding with the detection and quantification of wildfire disturbances on nature-based climate solutions.\"\n",
      "* 1_EpiClim Weekly District-Wise all-India multi-epidemics Climate-Health   Dataset for accelerated GeoH.pdf -> \"Figure 15 reveals distinct clustering patterns of disease cases based on temperature and precipitation.\"\n",
      "* 3_Climate And Resource Awareness is Imperative to Achieving Sustainable AI   and Preventing a Global A.pdf -> Not directly relevant to climate data analysis techniques, but mentions the importance of addressing fairness in AI for medical imaging, which could be related to climate health analysis.\n",
      "\n",
      "\n",
      "üîç REVIEWER CRITIQUE:\n",
      "--------------------------------------------------------------------------------\n",
      "‚Äî\n",
      "\n",
      "\n",
      "üí° COLLECTIVE INSIGHT:\n",
      "--------------------------------------------------------------------------------\n",
      "Collective Insight Report:\n",
      "\n",
      "The integration of AI and machine learning techniques in climate data analysis can reveal valuable insights into the relationships between climate factors and disease trends, supporting informed decision-making and automation. Climate data analysis can identify patterns and relationships between climate factors and disease trends, enabling strategies for mitigating climate change impacts. By leveraging techniques like computer vision and deep learning, climate data analysis can provide actionable insights for resource management and disease prevention.\n",
      "\n",
      "Testable hypotheses or follow-up experiments:\n",
      "1. Can the application of computer vision methods to satellite imagery improve the detection and quantification of climate-related disturbances, such as wildfires or droughts?\n",
      "2. How do temperature and precipitation patterns influence the clustering of disease cases, and can this knowledge be used to develop early warning systems for disease outbreaks?\n",
      "\n",
      "Relevant sources to test these hypotheses include:\n",
      "* 2_Towards AI-driven Integrative Emissions Monitoring  Management for   Nature-Based Climate Solutions.pdf\n",
      "* 1_EpiClim Weekly District-Wise all-India multi-epidemics Climate-Health   Dataset for accelerated GeoH.pdf\n",
      "These sources provide a foundation for exploring the use of AI and machine learning in climate data analysis, and can be supplemented with additional research on computer vision, deep learning, and climate-health relationships.\n",
      "\n",
      "\n",
      "üìö SOURCES USED:\n",
      "--------------------------------------------------------------------------------\n",
      "1. 3_Climate And Resource Awareness is Imperative to Achieving Sustainable AI   and Preventing a Global A.pdf\n",
      "2. 2_Towards AI-driven Integrative Emissions Monitoring  Management for   Nature-Based Climate Solutions.pdf\n",
      "3. 1_EpiClim Weekly District-Wise all-India multi-epidemics Climate-Health   Dataset for accelerated GeoH.pdf\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Create a simpler pipeline with just researcher and synthesizer (no reviewer)\n",
    "simple_pipeline = (ResearchPipelineBuilder(llm)\n",
    "    .with_retrieval_strategy(bm25_strategy)\n",
    "    .with_researcher(k=3)\n",
    "    .with_synthesizer()\n",
    "    .build())\n",
    "\n",
    "simple_assistant = ResearchAssistant(simple_pipeline, bm25_strategy)\n",
    "\n",
    "# Test the simple pipeline\n",
    "result3 = simple_assistant.research(\"Climate data analysis techniques\")\n",
    "simple_assistant.print_results(result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a63449e",
   "metadata": {},
   "source": [
    "## üìä Design Patterns Summary\n",
    "\n",
    "This notebook demonstrates several design patterns:\n",
    "\n",
    "### 1. **Strategy Pattern** (Retrieval Strategies)\n",
    "- **Interface**: `RetrievalStrategy`\n",
    "- **Implementations**: `BM25RetrievalStrategy`, `SemanticRetrievalStrategy` (placeholder)\n",
    "- **Benefit**: Easy to swap retrieval algorithms without changing agent code\n",
    "\n",
    "### 2. **Agent Pattern** (Agent Classes)\n",
    "- **Base Class**: `Agent`\n",
    "- **Implementations**: `ResearcherAgent`, `ReviewerAgent`, `SynthesizerAgent`\n",
    "- **Benefit**: Each agent has clear responsibilities and can be tested independently\n",
    "\n",
    "### 3. **Builder Pattern** (Pipeline Builder)\n",
    "- **Class**: `ResearchPipelineBuilder`\n",
    "- **Benefit**: Fluent API for constructing complex pipelines with different configurations\n",
    "\n",
    "### 4. **Facade Pattern** (Research Assistant)\n",
    "- **Class**: `ResearchAssistant`\n",
    "- **Benefit**: Simple high-level interface hiding complex pipeline details\n",
    "\n",
    "---\n",
    "\n",
    "## üîß How to Extend\n",
    "\n",
    "### Adding a New Retrieval Strategy:\n",
    "```python\n",
    "class HybridRetrievalStrategy(RetrievalStrategy):\n",
    "    def __init__(self, chunks):\n",
    "        self.bm25 = BM25RetrievalStrategy(chunks)\n",
    "        # Add semantic retriever here\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 3):\n",
    "        # Combine BM25 and semantic results\n",
    "        pass\n",
    "    \n",
    "    def get_strategy_name(self):\n",
    "        return \"Hybrid BM25 + Semantic Retrieval\"\n",
    "```\n",
    "\n",
    "### Adding a New Agent:\n",
    "```python\n",
    "class FactCheckerAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(llm, \"FactChecker\")\n",
    "    \n",
    "    def process(self, state: dict):\n",
    "        # Implement fact-checking logic\n",
    "        return {\"fact_check\": \"...\"}\n",
    "```\n",
    "\n",
    "### Building Custom Pipelines:\n",
    "```python\n",
    "custom_pipeline = (ResearchPipelineBuilder(llm)\n",
    "    .with_retrieval_strategy(custom_strategy)\n",
    "    .with_researcher(k=5)\n",
    "    .with_reviewer()\n",
    "    .with_custom_agent(FactCheckerAgent(llm))\n",
    "    .with_synthesizer()\n",
    "    .build())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2b45f",
   "metadata": {},
   "source": [
    "## üß™ Interactive Testing\n",
    "\n",
    "You can easily test different queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a152781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test function\n",
    "def quick_research(topic: str):\n",
    "    \"\"\"Quick research on any topic\"\"\"\n",
    "    result = assistant.research(topic)\n",
    "    assistant.print_results(result)\n",
    "    return result\n",
    "\n",
    "# Example: Test with your own query\n",
    "# Uncomment and modify the line below:\n",
    "# quick_research(\"Your research topic here\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
